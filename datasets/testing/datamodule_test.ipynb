{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets.datasets import SHHSdataset\n",
    "from datasets.datamodules import EEGdataModule\n",
    "import constants\n",
    "import pytorch_lightning as pl\n",
    "from models.supervised_model import SupervisedModel\n",
    "from models.mymodules import CNN_head\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dm = EEGdataModule(\n",
    "    data_path=constants.SHHS_PATH_DEKSTOP,\n",
    "    batch_size=64,\n",
    "    data_split=[4, 0],\n",
    "    num_workers=0,\n",
    "    num_patients=4,\n",
    "    num_ds=4,\n",
    "    first_patient=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'current_epoch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6172\\318617110.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdl\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mb1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\thesis\\datasets\\datamodules.py\u001B[0m in \u001B[0;36mtrain_dataloader\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     53\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_ds\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 55\u001B[1;33m             \u001B[0midx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcurrent_epoch\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_ds\u001B[0m  \u001B[1;31m# self.trainer.current_epoch % self.num_ds\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     56\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m         \u001B[1;31m# TODO: set shuffle to true!\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'current_epoch'"
     ]
    }
   ],
   "source": [
    "dl = dm.train_dataloader()\n",
    "b1 = list(iter(dl))[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dl = dm.train_dataloader()\n",
    "b2 = list(iter(dl))[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dl = dm.train_dataloader()\n",
    "b3 = list(iter(dl))[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dl = dm.train_dataloader()\n",
    "b4 = list(iter(dl))[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dl = dm.train_dataloader()\n",
    "b5 = list(iter(dl))[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6172\\2937418403.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb5\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mb1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'b5' is not defined"
     ]
    }
   ],
   "source": [
    "torch.sum(b5[1] - b1[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = dm.train_dataloader()\n",
    "b6 = list(iter(dl))[0]\n",
    "torch.sum(b6[0] - b2[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 1, 3000])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(0.),\n tensor(1.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(0.),\n tensor(1.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(3.),\n tensor(3.),\n tensor(3.),\n tensor(3.),\n tensor(2.),\n tensor(3.),\n tensor(3.),\n tensor(3.),\n tensor(3.),\n tensor(0.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(3.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.),\n tensor(2.)]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = SHHSdataset(\n",
    "    first_patient=1,\n",
    "    num_patients=1,\n",
    "    data_path=constants.SHHS_PATH_DEKSTOP\n",
    ")\n",
    "batch = [ds.__getitem__(x)[1] for x in range(64)]\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\tomsm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\tomsm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder     | CNN_head         | 6.2 M \n",
      "1 | classifier  | Linear           | 1.3 K \n",
      "2 | net         | Sequential       | 6.2 M \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.658    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cce3dbebabe4cdcb05c6813c0afa2f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TRAINER EPOCH:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\tomsm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8940c3510ce4860bbdbed5e3e993ce7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch idx:  0 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 0., 1., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 2., 3., 3., 3., 3., 0., 2.,\n",
      "        2., 2., 2., 3., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  1 -----------\n",
      "Labels:  tensor([2., 2., 2., 3., 3., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 2., 2., 3., 3., 2., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 2.,\n",
      "        3., 2., 2., 3., 2., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  2 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 0., 1., 1.,\n",
      "        2., 0., 1., 2., 2., 2., 2., 0., 1., 2., 2., 2., 0., 1., 1., 0., 1., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 0., 0., 1., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 0., 0., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  3 -----------\n",
      "Labels:  tensor([2., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 3., 3., 3., 2., 3., 0., 0., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 3., 3., 3., 3., 3., 3., 2.])\n",
      "Batch idx:  4 -----------\n",
      "Labels:  tensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 4., 4., 0.,\n",
      "        1., 2., 2., 2., 2., 2., 2., 2., 2., 4., 0., 1., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 0., 1., 2.,\n",
      "        2., 0., 0., 0., 0., 0., 0., 1., 2., 2.])\n",
      "Batch idx:  5 -----------\n",
      "Labels:  tensor([2., 2., 2., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 3., 3., 3., 3., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 2., 3., 3., 2., 3., 3., 3., 3.])\n",
      "Batch idx:  6 -----------\n",
      "Labels:  tensor([3., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 2., 2., 2., 2., 3., 3., 3., 3., 3., 2., 3.,\n",
      "        2., 3., 2., 2., 2., 2., 2., 2., 2., 3.])\n",
      "Batch idx:  7 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 2., 3., 2., 2., 2., 2., 0., 1., 2., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 1., 1., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  8 -----------\n",
      "Labels:  tensor([2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  9 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 3., 2.,\n",
      "        2., 2., 2., 2., 3., 2., 2., 2., 2., 0., 0., 2., 2., 2., 0., 1., 2., 0.,\n",
      "        0., 2., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 0., 2., 2., 2., 0.])\n",
      "Batch idx:  10 -----------\n",
      "Labels:  tensor([0., 1., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 1.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4.])\n",
      "Batch idx:  11 -----------\n",
      "Labels:  tensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0.,\n",
      "        0., 2., 0., 2., 2., 2., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 0., 0.])\n",
      "Batch idx:  12 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 1., 2., 2., 0., 2.,\n",
      "        2., 2., 2., 2., 0., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 0., 0., 2., 2.])\n",
      "Batch idx:  13 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 0., 1., 1., 1., 4., 4., 4., 4., 4., 4., 4., 4., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 2., 2., 2., 0., 0.])\n",
      "Batch idx:  14 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 2., 2., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  15 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7a5998a01b34f178bd314e307c0f52d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TRAINER EPOCH:  1\n",
      "Batch idx:  0 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 3., 2., 3.,\n",
      "        2., 0., 2., 2., 2., 2., 3., 2., 3., 3., 3., 3., 3., 3., 2., 3., 3., 3.,\n",
      "        3., 3., 3., 2., 3., 2., 3., 2., 3., 2.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch idx:  1 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 4., 2., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  2 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 1., 2., 0., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  3 -----------\n",
      "Labels:  tensor([2., 3., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 2., 3., 2., 2., 2., 3.,\n",
      "        3., 2., 2., 2., 2., 3., 3., 2., 3., 3., 2., 3., 3., 2., 2., 2., 2., 2.,\n",
      "        2., 3., 2., 3., 3., 3., 3., 3., 3., 3., 2., 2., 3., 2., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 2., 2., 3., 3., 2., 2., 2.])\n",
      "Batch idx:  4 -----------\n",
      "Labels:  tensor([3., 2., 3., 3., 2., 2., 3., 2., 2., 3., 2., 2., 3., 2., 2., 3., 2., 2.,\n",
      "        3., 2., 3., 3., 3., 2., 2., 2., 2., 2., 2., 2., 2., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  5 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 3., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  6 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 0., 1., 2.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.,\n",
      "        2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 0., 2., 2., 0., 0., 0.])\n",
      "Batch idx:  7 -----------\n",
      "Labels:  tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  8 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  9 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bbb6183f4e94d91a083b42b85ad3e4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TRAINER EPOCH:  2\n",
      "Batch idx:  0 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  1 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  2 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 2., 2., 2., 3., 2., 2., 2., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "Batch idx:  3 -----------\n",
      "Labels:  tensor([3., 0., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 0., 3., 3., 2., 2., 3., 2., 3., 3., 3., 3., 3., 2., 3.,\n",
      "        3., 3., 3., 0., 2., 2., 0., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "Batch idx:  4 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "Batch idx:  5 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 2., 2., 2., 0., 0., 0., 0., 2., 0.,\n",
      "        1., 1., 1., 1., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  6 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3.,\n",
      "        3., 3., 2., 2., 3., 2., 2., 2., 2., 2., 2., 0., 0., 0., 1., 1., 2., 2.,\n",
      "        2., 2., 3., 2., 2., 3., 3., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 2., 2.])\n",
      "Batch idx:  7 -----------\n",
      "Labels:  tensor([2., 2., 2., 4., 4., 4., 4., 4., 4., 2., 2., 2., 2., 2., 2., 2., 2., 0.,\n",
      "        1., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 4., 2.])\n",
      "Batch idx:  8 -----------\n",
      "Labels:  tensor([2., 2., 4., 4., 4., 4., 4., 2., 4., 4., 4., 4., 2., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 2., 0., 2., 0., 0., 1., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 2., 0., 2.,\n",
      "        0., 0., 0., 0., 0., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  9 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 2., 2., 2., 3., 2., 3., 2.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 0., 2., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 3., 3.,\n",
      "        3., 3., 3., 3., 2., 2., 3., 3., 3., 3.])\n",
      "Batch idx:  10 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 2., 3., 3., 2., 3., 3., 2., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 2., 2., 3., 2., 3., 2., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4.])\n",
      "Batch idx:  11 -----------\n",
      "Labels:  tensor([4., 4., 4., 4., 4., 2., 2., 4., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        0., 0., 0., 0., 1., 1., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  12 -----------\n",
      "Labels:  tensor([0., 1., 0., 0., 0., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 2., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  13 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 2., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        2., 2., 2., 2., 2., 2., 4., 4., 4., 4.])\n",
      "Batch idx:  14 -----------\n",
      "Labels:  tensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 2., 2., 2.,\n",
      "        0., 2., 2., 2., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 3., 3.])\n",
      "Batch idx:  15 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 0., 2., 2., 2., 2.])\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4f74947ba2d46b9b8a5cc09700a4520"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TRAINER EPOCH:  3\n",
      "Batch idx:  0 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsm\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch idx:  1 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2., 3., 2.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 3., 0., 1., 2., 3.,\n",
      "        2., 3., 3., 3., 3., 3., 2., 3., 3., 3.])\n",
      "Batch idx:  2 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 2., 2., 2., 2., 0., 0., 0.])\n",
      "Batch idx:  3 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "Batch idx:  4 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  5 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 3., 0., 2., 2., 3., 2., 2., 2., 2., 2., 0.,\n",
      "        2., 2., 2., 3., 2., 2., 2., 2., 3., 2., 2., 3., 3., 3., 2., 2., 3., 3.,\n",
      "        2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "Batch idx:  6 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 0., 0., 2., 2., 2., 3., 3.,\n",
      "        2., 2., 2., 3., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 4., 4.,\n",
      "        4., 4., 4., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  7 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 2., 0., 0., 0.,\n",
      "        0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 3., 3., 3., 2., 2.,\n",
      "        2., 3., 2., 2., 3., 2., 3., 2., 2., 3., 2., 3., 2., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "Batch idx:  8 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 2., 3., 3., 2., 2., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 0., 0., 2., 3., 2., 2., 2., 2., 2., 2., 3., 2., 2., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4.])\n",
      "Batch idx:  9 -----------\n",
      "Labels:  tensor([4., 0., 4., 4., 4., 4., 4., 0., 4., 4., 4., 4., 0., 4., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 3., 2., 0., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 3., 2.])\n",
      "Batch idx:  10 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 3., 2., 3., 2., 2., 3., 2., 2., 3., 2., 3., 3., 2.,\n",
      "        2., 3., 3., 3., 2., 2., 3., 3., 3., 3., 2., 2., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "Batch idx:  11 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 2., 0., 0.,\n",
      "        1., 4., 0., 1., 4., 4., 4., 0., 4., 0., 0., 0., 1., 1., 2., 2., 4., 4.,\n",
      "        0., 4., 4., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  12 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f284bc354e647dd9da78db8b604f59f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TRAINER EPOCH:  4\n",
      "Batch idx:  0 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 0., 1., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 2., 3., 3., 3., 3., 0., 2.,\n",
      "        2., 2., 2., 3., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  1 -----------\n",
      "Labels:  tensor([2., 2., 2., 3., 3., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 2., 2., 3., 3., 2., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 2.,\n",
      "        3., 2., 2., 3., 2., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  2 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 0., 1., 1.,\n",
      "        2., 0., 1., 2., 2., 2., 2., 0., 1., 2., 2., 2., 0., 1., 1., 0., 1., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 0., 0., 1., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 0., 0., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  3 -----------\n",
      "Labels:  tensor([2., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 3., 3., 3., 2., 3., 0., 0., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 3., 3., 3., 3., 3., 3., 2.])\n",
      "Batch idx:  4 -----------\n",
      "Labels:  tensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 4., 4., 0.,\n",
      "        1., 2., 2., 2., 2., 2., 2., 2., 2., 4., 0., 1., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 0., 1., 2.,\n",
      "        2., 0., 0., 0., 0., 0., 0., 1., 2., 2.])\n",
      "Batch idx:  5 -----------\n",
      "Labels:  tensor([2., 2., 2., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 3., 3., 3., 3., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 2., 3., 3., 2., 3., 3., 3., 3.])\n",
      "Batch idx:  6 -----------\n",
      "Labels:  tensor([3., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 2., 2., 2., 2., 3., 3., 3., 3., 3., 2., 3.,\n",
      "        2., 3., 2., 2., 2., 2., 2., 2., 2., 3.])\n",
      "Batch idx:  7 -----------\n",
      "Labels:  tensor([3., 3., 3., 3., 2., 3., 2., 2., 2., 2., 0., 1., 2., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 1., 1., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  8 -----------\n",
      "Labels:  tensor([2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Batch idx:  9 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 3., 2.,\n",
      "        2., 2., 2., 2., 3., 2., 2., 2., 2., 0., 0., 2., 2., 2., 0., 1., 2., 0.,\n",
      "        0., 2., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 0., 2., 2., 2., 0.])\n",
      "Batch idx:  10 -----------\n",
      "Labels:  tensor([0., 1., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 1.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4.])\n",
      "Batch idx:  11 -----------\n",
      "Labels:  tensor([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0.,\n",
      "        0., 2., 0., 2., 2., 2., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 0., 0.])\n",
      "Batch idx:  12 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 1., 2., 2., 0., 2.,\n",
      "        2., 2., 2., 2., 0., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 0., 0., 2., 2.])\n",
      "Batch idx:  13 -----------\n",
      "Labels:  tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 0., 1., 1., 1., 4., 4., 4., 4., 4., 4., 4., 4., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 2., 2., 2., 0., 0.])\n",
      "Batch idx:  14 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 2., 2., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch idx:  15 -----------\n",
      "Labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e78fb9e6a374ffbb1708921994f311b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    reload_dataloaders_every_n_epochs=1\n",
    ")\n",
    "model = SupervisedModel(\n",
    "    encoder=CNN_head(\n",
    "        conv_filters=[32, 64, 64],\n",
    "        representation_dim=256\n",
    "    ),\n",
    "    classifier=nn.Linear(256, 5),\n",
    "    optim_hparams={\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"lr_hparams\": None\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer.fit(model=model,\n",
    "            datamodule=dm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[-0.0064, -0.0430, -0.3476,  ..., -1.3511,  0.2365, -0.3168]],\n",
      "\n",
      "        [[ 0.1003,  0.8741, -0.2450,  ..., -1.0898, -1.1983, -0.0851]],\n",
      "\n",
      "        [[ 0.3587, -0.6339, -0.0508,  ..., -0.9387, -1.7475, -0.2455]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2211, -0.3582, -0.4797,  ..., -0.4645, -0.1535,  0.0269]],\n",
      "\n",
      "        [[ 0.2684,  0.2073, -0.0704,  ...,  0.4105, -0.0086, -0.6280]],\n",
      "\n",
      "        [[-1.0175, -1.1485, -0.5813,  ...,  0.3609,  0.6744,  1.3233]]]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 2., 2., 0., 1., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 2., 3., 3., 3., 3., 0., 2.,\n",
      "        2., 2., 2., 3., 2., 2., 2., 2., 2., 2.])]\n"
     ]
    }
   ],
   "source": [
    "dl = dm.val_dataloader()\n",
    "print(list(iter(dl))[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
