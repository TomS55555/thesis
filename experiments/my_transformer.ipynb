{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import constants\n",
    "import math\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "b, inner, outer, feat = 32, 5, 29, 128\n",
    "input = torch.randn(b, inner, outer, feat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "input_plus = input.view(b*inner, outer, feat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(input[1,1,:,:] - input_plus[6,:,:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_plus_min = input_plus.view(b, inner, outer, feat)\n",
    "torch.sum(input - input_plus_min)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 29, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder = nn.TransformerEncoderLayer(d_model=feat,\n",
    "                                     nhead=8,\n",
    "                                     dim_feedforward=1024,\n",
    "                                     batch_first=True)\n",
    "transformer_output = encoder(input_plus)\n",
    "print(transformer_output.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "        This class performs the final attention to reduce the dimensionality after the transformer encoder layer from [b+ x inner x feat] to [b+ x feat]\n",
    "        It does this by using attention which results in a weighted sum\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim, hidden_dim=None):\n",
    "        \"\"\"\n",
    "            hidden_dim is the hidden dimension used by attention\n",
    "            feat_dim is the feature dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = feat_dim\n",
    "        # self.Wa = torch.randn((hidden_dim, feat_dim), requires_grad=True)\n",
    "        # self.ba = torch.zeros(hidden_dim, requires_grad=True)\n",
    "        self.ae = nn.Parameter(torch.randn((hidden_dim,1), requires_grad=True))\n",
    "        self.linear = nn.Linear(in_features=feat_dim,\n",
    "                                out_features=hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        b, l, f = x.shape\n",
    "        #ats = self.tanh(x @ self.Wa.T + self.ba.repeat(l,1).unsqueeze(0).repeat(b,1,1))  # maybe this can faster\n",
    "        ats = self.tanh(self.linear(x))\n",
    "        alphas = self.softmax(ats @ self.ae)\n",
    "        result = torch.bmm(x.transpose(1,2), alphas).squeeze(2)\n",
    "        return result\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([160, 128])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = Aggregator(feat_dim=feat)\n",
    "agg(transformer_output).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "A, F = 9, 10\n",
    "Wa = torch.randn((A, F), requires_grad=True)\n",
    "ba = torch.zeros(A, requires_grad=True)\n",
    "ae = torch.randn((A,1), requires_grad=True)\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "b = 5  # batch size\n",
    "l = 3  # inner dimension\n",
    "batch = torch.randn(b, l, F)\n",
    "\n",
    "step1 = batch @ Wa.T\n",
    "print(step1.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "# Now add the bias row-wise\n",
    "row = torch.arange(A)\n",
    "rows = row.repeat(l,1).unsqueeze(0).repeat(b,1,1)\n",
    "print(rows.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "step2 = step1 + ba.repeat(l,1).unsqueeze(0).repeat(b,1,1)\n",
    "print(step2.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "step3 = tanh(step2)\n",
    "step4 = step3 @ ae\n",
    "print(step4.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "step5 = softmax(step4)\n",
    "print(step5.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Now it remains to make the appropriate sum with the original matrix\n",
    "step6 = torch.bmm(batch.transpose(1,2), step5).squeeze(2)\n",
    "print(step6.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class InnerTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        This module takes as input tensors of the form [batch x outer x inner x feat] and outputs tensors of the form [batch x outer x feat]\n",
    "        For the SleepTransformer, inner x feat is a time frequency image of a 1D EEG time-series (inner is time, feat is freq dimension)\n",
    "    \"\"\"\n",
    "    def __init__(self, inner_dim, feat_dim, dim_feedforward, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.dim_feedforward = dim_feedforward  # Size of hidden dimension used in MLP within transformerencoder layer\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=feat_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers)\n",
    "        self.aggregator = Aggregator(feat_dim=feat_dim)\n",
    "        self.inner_position_encoding = PositionalEncoding(\n",
    "            sequence_length=inner_dim,\n",
    "            hidden_size=feat_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_dim, outer_dim, inner_dim, feat_dim = x.shape\n",
    "        batch_plus = x.view(batch_dim*outer_dim, inner_dim, feat_dim)  # reshape before putting through transformer\n",
    "        batch_plus = self.inner_position_encoding(batch_plus)  # Add positional encoding\n",
    "        transformed_plus = self.transformer(batch_plus)\n",
    "        aggregrated_plus = self.aggregator(transformed_plus)\n",
    "        return aggregrated_plus.view(batch_dim, outer_dim, feat_dim)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'inner_dim'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6384\\1656766573.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minner\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mouter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m29\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m128\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minner\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mouter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m inner_transformer = InnerTransformer(feat_dim=feat,\n\u001B[0m\u001B[0;32m      4\u001B[0m                                      \u001B[0mdim_feedforward\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1024\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m                                      \u001B[0mnum_heads\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m8\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() missing 1 required positional argument: 'inner_dim'"
     ]
    }
   ],
   "source": [
    "b, inner, outer, feat = 32, 5, 29, 128\n",
    "input = torch.randn(b, inner, outer, feat)\n",
    "inner_transformer = InnerTransformer(feat_dim=feat,\n",
    "                                     dim_feedforward=1024,\n",
    "                                     num_heads=8,\n",
    "                                     num_layers=4)\n",
    "output = inner_transformer(input)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "class OuterTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        This module takes as input tensors of the form [batch x outer x feat] and outputs a tensors of\n",
    "        the form [batch x outer x feat] that can be used as input to a classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, outer_dim, feat_dim, dim_feedforward, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=feat_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.outer_position_encoding = PositionalEncoding(\n",
    "            sequence_length=outer_dim,\n",
    "            hidden_size=feat_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(self.outer_position_encoding(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'outer_dim'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6384\\3224102731.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m outer_transformer = OuterTransformer(feat_dim=feat,\n\u001B[0m\u001B[0;32m      2\u001B[0m                                      \u001B[0mdim_feedforward\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1024\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m                                      \u001B[0mnum_heads\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m8\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m                                      num_layers=4)\n\u001B[0;32m      5\u001B[0m \u001B[0mouter_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mouter_transformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() missing 1 required positional argument: 'outer_dim'"
     ]
    }
   ],
   "source": [
    "outer_transformer = OuterTransformer(feat_dim=feat,\n",
    "                                     dim_feedforward=1024,\n",
    "                                     num_heads=8,\n",
    "                                     num_layers=4)\n",
    "outer_output = outer_transformer(output)\n",
    "print(outer_output.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "        This module takes as input tensors of the form [batch x outer x feat] and outputs tensors of\n",
    "        the form\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=feat_dim,\n",
    "                out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_dim,\n",
    "                out_features=constants.N_CLASSES\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "classifier = Classifier(feat_dim=feat,\n",
    "                        hidden_dim=1024)\n",
    "classifier_output = classifier(outer_output)\n",
    "print(classifier_output.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "class SleepTransformer(pl.LightningModule):\n",
    "    \"\"\"\n",
    "        The SleepTransformer takes as input batches of time series of the form [batch x window x time] and outputs labels for each epoch: [batch x window x label]\n",
    "    \"\"\"\n",
    "    def __init__(self, outer_dim, inner_dim, feat_dim, dim_feedforward, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        inner_transformer = InnerTransformer(feat_dim=feat_dim,\n",
    "                                                  inner_dim=inner_dim,\n",
    "                                             dim_feedforward=dim_feedforward,\n",
    "                                             num_heads=num_heads,\n",
    "                                             num_layers=num_layers)\n",
    "        outer_transformer = OuterTransformer(feat_dim=feat_dim,\n",
    "                                                  outer_dim=outer_dim,\n",
    "                                             dim_feedforward=dim_feedforward,\n",
    "                                             num_heads=num_heads,\n",
    "                                             num_layers=num_layers)\n",
    "\n",
    "        classifier = Classifier(feat_dim=feat_dim,\n",
    "                                     hidden_dim=dim_feedforward)\n",
    "\n",
    "        self.net = nn.Sequential(inner_transformer,\n",
    "                                 outer_transformer,\n",
    "                                 classifier)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0572,  0.1763, -0.1210,  0.1096,  0.1139],\n",
      "         [-0.0537, -0.0031, -0.1423,  0.1247,  0.1492],\n",
      "         [ 0.1174,  0.0579, -0.3153,  0.1934,  0.1110],\n",
      "         [ 0.0488,  0.0058, -0.2966,  0.2370,  0.0303],\n",
      "         [ 0.0709,  0.0361, -0.0958,  0.2541,  0.0894]],\n",
      "\n",
      "        [[ 0.0845,  0.0899, -0.2799,  0.1379, -0.0215],\n",
      "         [-0.0017, -0.0315, -0.2627,  0.1713,  0.0042],\n",
      "         [-0.0870,  0.1827, -0.3365,  0.0996,  0.0747],\n",
      "         [ 0.0353,  0.0851, -0.2914,  0.1960,  0.1436],\n",
      "         [ 0.0257,  0.0687, -0.2340,  0.1568,  0.0506]],\n",
      "\n",
      "        [[-0.0827,  0.0610, -0.1491,  0.2429,  0.2998],\n",
      "         [-0.0006,  0.2079, -0.0657,  0.1148,  0.1149],\n",
      "         [-0.1578,  0.0996, -0.0410,  0.2496,  0.3133],\n",
      "         [ 0.0598,  0.1035, -0.1630,  0.0992,  0.3429],\n",
      "         [ 0.0202,  0.1283, -0.2296,  0.2161,  0.2460]],\n",
      "\n",
      "        [[-0.0375,  0.0739, -0.1890,  0.2242,  0.1991],\n",
      "         [-0.0170,  0.1516, -0.2030,  0.2181,  0.1715],\n",
      "         [-0.0008,  0.0701, -0.2443,  0.1847,  0.2526],\n",
      "         [ 0.0934,  0.0611, -0.1306,  0.2157,  0.2325],\n",
      "         [-0.0107, -0.0273, -0.3011,  0.0928,  0.2241]],\n",
      "\n",
      "        [[ 0.0630,  0.0615, -0.2095,  0.2472,  0.0922],\n",
      "         [-0.0416,  0.0206, -0.1599,  0.1787,  0.1815],\n",
      "         [ 0.0823,  0.0140, -0.2113,  0.1078, -0.0645],\n",
      "         [ 0.0906,  0.1102, -0.1553,  0.1981,  0.1442],\n",
      "         [ 0.2169,  0.0360, -0.1573,  0.0067,  0.0814]],\n",
      "\n",
      "        [[ 0.0888, -0.0819, -0.1228,  0.2075,  0.0146],\n",
      "         [-0.0455,  0.0362, -0.1276,  0.2405,  0.2285],\n",
      "         [-0.0916,  0.0444, -0.2291,  0.2505,  0.0798],\n",
      "         [-0.0685, -0.1106, -0.1891,  0.1727,  0.0665],\n",
      "         [-0.1807,  0.0788, -0.1595,  0.2039,  0.0367]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b, outer, inner, feat = 6, 5, 29, 128\n",
    "input_batch = torch.randn(b, outer, inner, feat)\n",
    "sleep_transformer = SleepTransformer(\n",
    "    outer_dim=outer,\n",
    "    inner_dim=inner,\n",
    "    feat_dim=feat,\n",
    "    dim_feedforward=1024,\n",
    "    num_heads=8,\n",
    "    num_layers=4\n",
    ")\n",
    "output_batch = sleep_transformer(input_batch)\n",
    "print(output_batch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stft(torch.FloatTensor[60, 3056], n_fft=56, hop_length=23, win_length=256, window=torch.FloatTensor{[256]}, normalized=0, onesided=1, return_complex=None) : expected 0 < win_length <= n_fft, but got win_length=256",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6384\\2420813194.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mwindow_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m batch_stft = torch.stft(time_series_plus,\n\u001B[0m\u001B[0;32m     10\u001B[0m                         \u001B[0mwin_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m256\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m                         \u001B[0mwindow\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhamming_window\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m256\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py\u001B[0m in \u001B[0;36mstft\u001B[1;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001B[0m\n\u001B[0;32m    630\u001B[0m         \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mextended_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mpad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpad\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpad_mode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    631\u001B[0m         \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0msignal_dim\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 632\u001B[1;33m     return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n\u001B[0m\u001B[0;32m    633\u001B[0m                     normalized, onesided, return_complex)\n\u001B[0;32m    634\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stft(torch.FloatTensor[60, 3056], n_fft=56, hop_length=23, win_length=256, window=torch.FloatTensor{[256]}, normalized=0, onesided=1, return_complex=None) : expected 0 < win_length <= n_fft, but got win_length=256"
     ]
    }
   ],
   "source": [
    "# Test STFT\n",
    "b, w, t = 12, 5, 3000\n",
    "time_series = torch.randn(b, w, t)\n",
    "time_series_plus = time_series.view(b*w, t)\n",
    "\n",
    "# define STFT parameters\n",
    "window_size = 4\n",
    "\n",
    "batch_stft = torch.stft(time_series_plus,\n",
    "                        win_length=256,\n",
    "                        window=torch.hamming_window(256),\n",
    "                        hop_length=23,\n",
    "                        n_fft=56,\n",
    "                        onesided=True\n",
    "                        )\n",
    "print(batch_stft.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.]]])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = torch.ones(10,5)\n",
    "begin = torch.zeros(3,10,5)\n",
    "begin+encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, sequence_length, hidden_size):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoding = self.get_positional_encoding()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding\n",
    "        return x\n",
    "\n",
    "    def get_positional_encoding(self):\n",
    "        # create a matrix of shape (sequence_length, hidden_size)\n",
    "        position = torch.arange(self.sequence_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.hidden_size, 2) * -(math.log(10000.0) / self.hidden_size))\n",
    "        sin = torch.sin(position * div_term)\n",
    "        cos = torch.cos(position * div_term)\n",
    "        encoding = torch.cat([sin, cos], dim=1)\n",
    "        return encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04,\n           5.4030e-01,  9.8747e-01,  9.9968e-01,  9.9999e-01,  1.0000e+00],\n         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03,\n          -4.1615e-01,  9.5018e-01,  9.9874e-01,  9.9997e-01,  1.0000e+00],\n         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03,\n          -9.8999e-01,  8.8908e-01,  9.9716e-01,  9.9993e-01,  1.0000e+00],\n         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03,\n          -6.5364e-01,  8.0569e-01,  9.9496e-01,  9.9987e-01,  1.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04,\n           5.4030e-01,  9.8747e-01,  9.9968e-01,  9.9999e-01,  1.0000e+00],\n         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03,\n          -4.1615e-01,  9.5018e-01,  9.9874e-01,  9.9997e-01,  1.0000e+00],\n         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03,\n          -9.8999e-01,  8.8908e-01,  9.9716e-01,  9.9993e-01,  1.0000e+00],\n         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03,\n          -6.5364e-01,  8.0569e-01,  9.9496e-01,  9.9987e-01,  1.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04,\n           5.4030e-01,  9.8747e-01,  9.9968e-01,  9.9999e-01,  1.0000e+00],\n         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03,\n          -4.1615e-01,  9.5018e-01,  9.9874e-01,  9.9997e-01,  1.0000e+00],\n         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03,\n          -9.8999e-01,  8.8908e-01,  9.9716e-01,  9.9993e-01,  1.0000e+00],\n         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03,\n          -6.5364e-01,  8.0569e-01,  9.9496e-01,  9.9987e-01,  1.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04,\n           5.4030e-01,  9.8747e-01,  9.9968e-01,  9.9999e-01,  1.0000e+00],\n         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03,\n          -4.1615e-01,  9.5018e-01,  9.9874e-01,  9.9997e-01,  1.0000e+00],\n         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03,\n          -9.8999e-01,  8.8908e-01,  9.9716e-01,  9.9993e-01,  1.0000e+00],\n         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03,\n          -6.5364e-01,  8.0569e-01,  9.9496e-01,  9.9987e-01,  1.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04,\n           5.4030e-01,  9.8747e-01,  9.9968e-01,  9.9999e-01,  1.0000e+00],\n         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03,\n          -4.1615e-01,  9.5018e-01,  9.9874e-01,  9.9997e-01,  1.0000e+00],\n         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03,\n          -9.8999e-01,  8.8908e-01,  9.9716e-01,  9.9993e-01,  1.0000e+00],\n         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03,\n          -6.5364e-01,  8.0569e-01,  9.9496e-01,  9.9987e-01,  1.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04,\n           5.4030e-01,  9.8747e-01,  9.9968e-01,  9.9999e-01,  1.0000e+00],\n         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03,\n          -4.1615e-01,  9.5018e-01,  9.9874e-01,  9.9997e-01,  1.0000e+00],\n         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03,\n          -9.8999e-01,  8.8908e-01,  9.9716e-01,  9.9993e-01,  1.0000e+00],\n         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03,\n          -6.5364e-01,  8.0569e-01,  9.9496e-01,  9.9987e-01,  1.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04,\n           5.4030e-01,  9.8747e-01,  9.9968e-01,  9.9999e-01,  1.0000e+00],\n         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03,\n          -4.1615e-01,  9.5018e-01,  9.9874e-01,  9.9997e-01,  1.0000e+00],\n         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03,\n          -9.8999e-01,  8.8908e-01,  9.9716e-01,  9.9993e-01,  1.0000e+00],\n         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03,\n          -6.5364e-01,  8.0569e-01,  9.9496e-01,  9.9987e-01,  1.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  1.5783e-01,  2.5116e-02,  3.9811e-03,  6.3096e-04,\n           5.4030e-01,  9.8747e-01,  9.9968e-01,  9.9999e-01,  1.0000e+00],\n         [ 9.0930e-01,  3.1170e-01,  5.0217e-02,  7.9621e-03,  1.2619e-03,\n          -4.1615e-01,  9.5018e-01,  9.9874e-01,  9.9997e-01,  1.0000e+00],\n         [ 1.4112e-01,  4.5775e-01,  7.5285e-02,  1.1943e-02,  1.8929e-03,\n          -9.8999e-01,  8.8908e-01,  9.9716e-01,  9.9993e-01,  1.0000e+00],\n         [-7.5680e-01,  5.9234e-01,  1.0031e-01,  1.5924e-02,  2.5238e-03,\n          -6.5364e-01,  8.0569e-01,  9.9496e-01,  9.9987e-01,  1.0000e+00]]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding = PositionalEncoding(sequence_length=5, hidden_size=10)\n",
    "begin = torch.zeros(8,5,10)\n",
    "pos_encoding(begin)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
