{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "b, inner, outer, feat = 32, 5, 29, 128\n",
    "input = torch.randn(b, inner, outer, feat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "input_plus = input.view(b*inner, outer, feat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(input[1,1,:,:] - input_plus[6,:,:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_plus_min = input_plus.view(b, inner, outer, feat)\n",
    "torch.sum(input - input_plus_min)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 29, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder = nn.TransformerEncoderLayer(d_model=feat,\n",
    "                                     nhead=8,\n",
    "                                     dim_feedforward=1024,\n",
    "                                     batch_first=True)\n",
    "transformer_output = encoder(input_plus)\n",
    "print(transformer_output.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "        This class performs the final attention to reduce the dimensionality after the transformer encoder layer from [b+ x inner x feat] to [b+ x feat]\n",
    "        It does this by using attention which results in a weighted sum\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim, hidden_dim=None):\n",
    "        \"\"\"\n",
    "            hidden_dim is the hidden dimension used by attention\n",
    "            feat_dim is the feature dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = feat_dim\n",
    "        self.Wa = torch.randn((hidden_dim, feat_dim), requires_grad=True)\n",
    "        self.ba = torch.zeros(hidden_dim, requires_grad=True)\n",
    "        self.ae = torch.randn((hidden_dim,1), requires_grad=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        b, l, f = x.shape\n",
    "        ats = self.tanh(x @ self.Wa.T + self.ba.repeat(l,1).unsqueeze(0).repeat(b,1,1))  # maybe this can faster\n",
    "        alphas = self.softmax(ats @ self.ae)\n",
    "        result = torch.bmm(x.transpose(1,2), alphas).squeeze(2)\n",
    "        return result\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([160, 128])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = Aggregator(feat_dim=feat)\n",
    "agg(transformer_output).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "A, F = 9, 10\n",
    "Wa = torch.randn((A, F), requires_grad=True)\n",
    "ba = torch.zeros(A, requires_grad=True)\n",
    "ae = torch.randn((A,1), requires_grad=True)\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "b = 5  # batch size\n",
    "l = 3  # inner dimension\n",
    "batch = torch.randn(b, l, F)\n",
    "\n",
    "step1 = batch @ Wa.T\n",
    "print(step1.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "# Now add the bias row-wise\n",
    "row = torch.arange(A)\n",
    "rows = row.repeat(l,1).unsqueeze(0).repeat(b,1,1)\n",
    "print(rows.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "step2 = step1 + ba.repeat(l,1).unsqueeze(0).repeat(b,1,1)\n",
    "print(step2.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "step3 = tanh(step2)\n",
    "step4 = step3 @ ae\n",
    "print(step4.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "step5 = softmax(step4)\n",
    "print(step5.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Now it remains to make the appropriate sum with the original matrix\n",
    "step6 = torch.bmm(batch.transpose(1,2), step5).squeeze(2)\n",
    "print(step6.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 10])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class InnerTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        This module takes as input tensors of the form [batch x outer x inner x feat] and outputs tensors of the form [batch x outer x feat]\n",
    "        For the SleepTransformer, inner x feat is a time frequency image of a 1D EEG time-series (inner is time, feat is freq dimension)\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim, dim_feedforward, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.dim_feedforward = dim_feedforward  # Size of hidden dimension used in MLP within transformerencoder layer\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=feat_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers)\n",
    "        self.aggregator = Aggregator(feat_dim=feat_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_dim, outer_dim, inner_dim, feat_dim = x.shape\n",
    "        batch_plus = x.view(batch_dim*outer_dim, inner_dim, feat_dim)  # reshape before putting through transformer\n",
    "        transformed_plus = self.transformer(batch_plus)\n",
    "        aggregrated_plus = self.aggregator(transformed_plus)\n",
    "        return aggregrated_plus.view(batch_dim, outer_dim, feat_dim)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "b, inner, outer, feat = 32, 5, 29, 128\n",
    "input = torch.randn(b, inner, outer, feat)\n",
    "inner_transformer = InnerTransformer(feat_dim=feat,\n",
    "                                     dim_feedforward=1024,\n",
    "                                     num_heads=8,\n",
    "                                     num_layers=4)\n",
    "output = inner_transformer(input)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class OuterTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        This module takes as input tensors of the form [batch x outer x feat] and outputs a tensors of\n",
    "        the form [batch x outer x feat] that can be used as input to a classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim, dim_feedforward, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=feat_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "outer_transformer = OuterTransformer(feat_dim=feat,\n",
    "                                     dim_feedforward=1024,\n",
    "                                     num_heads=8,\n",
    "                                     num_layers=4)\n",
    "outer_output = outer_transformer(output)\n",
    "print(outer_output.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "        This module takes as input tensors of the form [batch x outer x feat] and outputs tensors of\n",
    "        the form\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=feat_dim,\n",
    "                out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_dim,\n",
    "                out_features=constants.N_CLASSES\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Classifier\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
