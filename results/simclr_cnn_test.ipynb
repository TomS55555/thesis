{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.extend(['/users/students/r0749898/thesis/'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datasets.SHHS_dataset_timeonly import SHHS_dataset_1, EEGdataModule\n",
    "\n",
    "from models.simclr_model import SimCLR\n",
    "from models.supervised_model import SupervisedModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from copy import deepcopy\n",
    "from utils.helper_functions import load_model, SimCLRdataModule\n",
    "from trainers.train_supervised import train_supervised\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dataset: 10 patients for training, 5 for validation and 30 for testing\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find file at path:  /esat/biomeddata/SHHS_Dataset/no_backup/n0068_eeg.mat\n",
      "Couldn't find file at path:  /esat/biomeddata/SHHS_Dataset/no_backup/n0086_eeg.mat\n",
      "Couldn't find file at path:  /esat/biomeddata/SHHS_Dataset/no_backup/n0094_eeg.mat\n"
     ]
    }
   ],
   "source": [
    "data_args = {\n",
    "  \"DATA_PATH\": \"/esat/biomeddata/SHHS_Dataset/no_backup/\",\n",
    "  \"data_split\": [2, 1],\n",
    "  \"first_patient\": 15,\n",
    "  \"num_patients_train\": 50,\n",
    "  \"num_patients_test\": 30,\n",
    "  \"batch_size\": 64,\n",
    "  \"num_workers\": 12\n",
    "}\n",
    "\n",
    "dm = EEGdataModule(**data_args)  # Load datamodule\n",
    "dm.setup()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run data through the pretrained SimCLR encoder to get the representations\n",
    "- The SimCLR model was pretrained on 100 patients (+50 for validation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_path = \"../trained_models/cnn_simclr01.ckpt\"\n",
    "pretrained_model = load_model(SimCLR, encoder_path)  # Load pretrained simclr model\n",
    "simclr_dm = SimCLRdataModule(pretrained_model, dm, data_args['batch_size'], data_args['num_workers'], device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Analyse the features: histogram and t-SNE plot\n",
    "feature = next(iter(simclr_dm.train_dataloader()))[0][0]\n",
    "plt.hist(np.asarray(feature), bins=50)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a logistic classifier on top\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logistic_args = {\n",
    "  \"MODEL_TYPE\": \"SupervisedModel\",\n",
    "  \"save_name\": \"logistic_on_simclr\",\n",
    "  \"DATA_PATH\": data_args['DATA_PATH'],\n",
    "  \"CHECKPOINT_PATH\": \"checkpoints\",\n",
    "\n",
    "  \"encoder\": \"None\",\n",
    "  \"encoder_hparams\": {},\n",
    "\n",
    "  \"classifier\": \"logistic\",\n",
    "  \"classifier_hparams\":{\n",
    "      \"input_dim\": 100\n",
    "  },\n",
    "  \"data_hparams\": data_args,\n",
    "\n",
    "  \"trainer_hparams\":{\n",
    "    \"max_epochs\": 15\n",
    "  },\n",
    "  \"optim_hparams\": {\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4\n",
    "  }\n",
    "}\n",
    "logistic_model, logistic_res = train_supervised(Namespace(**logistic_args), device=device, dm=simclr_dm)\n",
    "print(logistic_res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a supervised model with the same dataset for comparison\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find file at path:  /esat/biomeddata/SHHS_Dataset/no_backup/n0068_eeg.mat\n",
      "Couldn't find file at path:  /esat/biomeddata/SHHS_Dataset/no_backup/n0086_eeg.mat\n",
      "Couldn't find file at path:  /esat/biomeddata/SHHS_Dataset/no_backup/n0094_eeg.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/users/students/r0749898/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:268: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "  rank_zero_warn(\n",
      "/users/students/r0749898/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:268: UserWarning: Attribute 'classifier' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['classifier'])`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: checkpoints/supervised_simclr/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AdamW.__init__() got an unexpected keyword argument 'lr_hparams'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 28\u001B[0m\n\u001B[1;32m      1\u001B[0m supervised_args \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      2\u001B[0m   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMODEL_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSupervisedModel\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msupervised_simclr\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m   }\n\u001B[1;32m     27\u001B[0m }\n\u001B[0;32m---> 28\u001B[0m sup_model, sup_res \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_supervised\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNamespace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msupervised_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(sup_res)\n",
      "File \u001B[0;32m~/thesis/trainers/train_supervised.py:39\u001B[0m, in \u001B[0;36mtrain_supervised\u001B[0;34m(args, device, pretrained_encoder, pretrained_classifier, dm)\u001B[0m\n\u001B[1;32m     36\u001B[0m classifier \u001B[38;5;241m=\u001B[39m constants\u001B[38;5;241m.\u001B[39mCLASSIFIERS[args\u001B[38;5;241m.\u001B[39mclassifier](\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39margs\u001B[38;5;241m.\u001B[39mclassifier_hparams) \u001B[38;5;28;01mif\u001B[39;00m pretrained_classifier \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m pretrained_classifier\n\u001B[1;32m     38\u001B[0m model \u001B[38;5;241m=\u001B[39m SupervisedModel(encoder, classifier, args\u001B[38;5;241m.\u001B[39moptim_hparams)\n\u001B[0;32m---> 39\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mval_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m model \u001B[38;5;241m=\u001B[39m SupervisedModel\u001B[38;5;241m.\u001B[39mload_from_checkpoint(\n\u001B[1;32m     42\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mcheckpoint_callback\u001B[38;5;241m.\u001B[39mbest_model_path)  \u001B[38;5;66;03m# Load best checkpoint after training\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Test best model on validation and test set\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:696\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    677\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;124;03mRuns the full optimization routine.\u001B[39;00m\n\u001B[1;32m    679\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001B[39;00m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 696\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    648\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    649\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 650\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m \u001B[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001B[39;00m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:735\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    731\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    732\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_ckpt_path(\n\u001B[1;32m    733\u001B[0m     ckpt_path, model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    734\u001B[0m )\n\u001B[0;32m--> 735\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1147\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1144\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logger_connector\u001B[38;5;241m.\u001B[39mreset_metrics()\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;66;03m# strategy will configure model and move it to the device\u001B[39;00m\n\u001B[0;32m-> 1147\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1149\u001B[0m \u001B[38;5;66;03m# hook\u001B[39;00m\n\u001B[1;32m   1150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;241m==\u001B[39m TrainerFn\u001B[38;5;241m.\u001B[39mFITTING:\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py:74\u001B[0m, in \u001B[0;36mSingleDeviceStrategy.setup\u001B[0;34m(self, trainer)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msetup\u001B[39m(\u001B[38;5;28mself\u001B[39m, trainer: pl\u001B[38;5;241m.\u001B[39mTrainer) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_to_device()\n\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:153\u001B[0m, in \u001B[0;36mStrategy.setup\u001B[0;34m(self, trainer)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39msetup(trainer)\n\u001B[0;32m--> 153\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup_optimizers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_precision_plugin()\n\u001B[1;32m    155\u001B[0m optimizers_to_device(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_device)\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:141\u001B[0m, in \u001B[0;36mStrategy.setup_optimizers\u001B[0;34m(self, trainer)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_scheduler_configs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_frequencies \u001B[38;5;241m=\u001B[39m \u001B[43m_init_optimizers_and_lr_schedulers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:179\u001B[0m, in \u001B[0;36m_init_optimizers_and_lr_schedulers\u001B[0;34m(model)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_init_optimizers_and_lr_schedulers\u001B[39m(\n\u001B[1;32m    176\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    177\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[Optimizer], List[LRSchedulerConfig], List[\u001B[38;5;28mint\u001B[39m]]:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;124;03m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m     optim_conf \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfigure_optimizers\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpl_module\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m optim_conf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    182\u001B[0m         rank_zero_warn(\n\u001B[1;32m    183\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    184\u001B[0m         )\n",
      "File \u001B[0;32m~/miniconda3/envs/newthesisenv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1550\u001B[0m, in \u001B[0;36mTrainer._call_lightning_module_hook\u001B[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1547\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m   1549\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1550\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1553\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/thesis/models/supervised_model.py:25\u001B[0m, in \u001B[0;36mSupervisedModel.configure_optimizers\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconfigure_optimizers\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m---> 25\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m \u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdamW\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim_hparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;66;03m# We will reduce the learning rate by 0.1 after 100 and 150 epochs\u001B[39;00m\n\u001B[1;32m     28\u001B[0m     scheduler \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mMultiStepLR(\n\u001B[1;32m     29\u001B[0m         optimizer, milestones\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m150\u001B[39m], gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: AdamW.__init__() got an unexpected keyword argument 'lr_hparams'"
     ]
    }
   ],
   "source": [
    "supervised_args = {\n",
    "  \"MODEL_TYPE\": \"SupervisedModel\",\n",
    "  \"save_name\": \"supervised_simclr\",\n",
    "  \"DATA_PATH\": data_args['DATA_PATH'],\n",
    "  \"CHECKPOINT_PATH\": \"checkpoints\",\n",
    "\n",
    "  \"encoder\": \"CNN_head\",\n",
    "  \"encoder_hparams\": {\n",
    "    \"conv_filters\": [32, 64, 64],\n",
    "    \"representation_dim\": 100\n",
    "  },\n",
    "\n",
    "  \"classifier\": \"logistic\",\n",
    "  \"classifier_hparams\":{\n",
    "      \"input_dim\": 100\n",
    "  },\n",
    "  \"data_hparams\": data_args,\n",
    "\n",
    "  \"trainer_hparams\":{\n",
    "    \"max_epochs\": 40\n",
    "  },\n",
    "  \"optim_hparams\": {\n",
    "    \"lr\": 1e-5,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"lr_hparams\": None\n",
    "  }\n",
    "}\n",
    "sup_model, sup_res = train_supervised(Namespace(**supervised_args), device, dm=dm)\n",
    "print(sup_res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetuned supervised model"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finetune_args = {\n",
    "  \"MODEL_TYPE\": \"SupervisedModel\",\n",
    "  \"save_name\": \"finetuned_simclr\",\n",
    "  \"DATA_PATH\": data_args['DATA_PATH'],\n",
    "  \"CHECKPOINT_PATH\": \"checkpoints\",\n",
    "\n",
    "  \"encoder\": \"CNN_head\",\n",
    "  \"encoder_hparams\": {\n",
    "    \"conv_filters\": [32, 64, 64],\n",
    "    \"representation_dim\": 100\n",
    "  },\n",
    "\n",
    "  \"classifier\": \"logistic\",\n",
    "  \"classifier_hparams\":{\n",
    "      \"input_dim\": 100\n",
    "  },\n",
    "  \"data_hparams\": data_args,\n",
    "\n",
    "  \"trainer_hparams\":{\n",
    "    \"max_epochs\": 30\n",
    "  },\n",
    "  \"optim_hparams\": {\n",
    "    \"lr\": 1e-5,\n",
    "    \"weight_decay\": 1e-5\n",
    "  }\n",
    "}\n",
    "pretrained_encoder = type(pretrained_model.f)(**finetune_args['encoder_hparams'])\n",
    "pretrained_encoder.load_state_dict(pretrained_model.f.state_dict())\n",
    "fine_tuned_model, fine_tuned_res = train_supervised(Namespace(**finetune_args), device, dm=dm, pretrained_encoder=pretrained_encoder)\n",
    "print(fine_tuned_res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finetune_logistic_args = {\n",
    "  \"MODEL_TYPE\": \"SupervisedModel\",\n",
    "  \"save_name\": \"finetuned_simclr\",\n",
    "  \"DATA_PATH\": data_args['DATA_PATH'],\n",
    "  \"CHECKPOINT_PATH\": \"checkpoints\",\n",
    "\n",
    "  \"encoder\": \"CNN_head\",\n",
    "  \"encoder_hparams\": {\n",
    "    \"conv_filters\": [32, 64, 64],\n",
    "    \"representation_dim\": 100\n",
    "  },\n",
    "\n",
    "  \"classifier\": \"logistic\",\n",
    "  \"classifier_hparams\":{\n",
    "      \"input_dim\": 100\n",
    "  },\n",
    "  \"data_hparams\": data_args,\n",
    "\n",
    "  \"trainer_hparams\":{\n",
    "    \"max_epochs\": 50\n",
    "  },\n",
    "  \"optim_hparams\": {\n",
    "    \"lr\": 1e-6,\n",
    "    \"weight_decay\": 0\n",
    "  }\n",
    "}\n",
    "pretrained_encoder = type(pretrained_model.f)(**finetune_logistic_args['encoder_hparams'])\n",
    "pretrained_classifier = type(logistic_model.classifier)(finetune_logistic_args['classifier_hparams']['input_dim'], 5)\n",
    "pretrained_classifier.load_state_dict(logistic_model.classifier.state_dict())\n",
    "pretrained_encoder.load_state_dict(pretrained_model.f.state_dict())\n",
    "fully_tuned_model, fully_tuned_res = train_supervised(Namespace(**finetune_logistic_args), device, dm=dm, pretrained_encoder=pretrained_encoder, pretrained_classifier=pretrained_classifier)\n",
    "print(fully_tuned_res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(fully_tuned_res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
